#import "@preview/glossarium:0.5.1": gls, glspl

= Materials <materials.sec>
This chapter describes the materials used in the study, including the datasets, tools, and resources that were essential for conducting the experiments and analyses. 

== spaCy <spacy.sec>
Spacy is an open-source software library for advanced #gls("NLP") in Python. It is designed specifically for production use and is widely used in industry. Spacy provides a wide range of features for processing text data, including tokenization, #gls("POS") tagging, named entity recognition, and dependency parsing. It also includes pre-trained models for a variety of languages and domains, making it easy to get started with #gls("NLP") tasks. The relevant features of spaCy for this thesis are the #gls("POS") tagger and morphologizer, lemmatizer and dependency parser.

== NanoT5 <nanot5.sec>
The Nano#gls("T5") framework, introduced by @nawrot, addresses the computational challenges associated with training large-scale Transformer models like #gls("T5"). Recognizing that the substantial resource requirements of such models can hinder research accessibility, Nano#gls("T5") offers a streamlined, PyTorch-based solution for efficient pre-training and fine-tuning of #gls("T5")-style models, particularly under constrained computational budgets.
The primary motivation behind Nano#gls("T5") is to democratize access to advanced language modeling by reducing the computational barriers to training #gls("T5") models. While existing implementations of #gls("T5") often rely on resource-intensive frameworks like JAX/Flax, Nano#gls("T5") provides a PyTorch-based alternative that emphasizes simplicity and efficiency. By optimizing various components of the training pipeline, Nano#gls("T5") enables researchers to pre-train a #gls("T5")-Base model on a single GPU within approximately 16 hours, without compromising performance.
The key features of Nano#gls("T5") include:
- *On the fly data processing* 
- *Flexible optimizer support* The original Adafactor optimizer is supported, but the framework also allows for the use of other optimizers like AdamW.
- *Fine-tuning* Nano#gls("T5") supports fine-tuning on downstream tasks, enabling users to adapt pre-trained models to specific applications. The framework has been applied to #gls("SNI") benchmark, demonstrating its effectiveness in fine-tuning tasks.
- *Accessibility and reproducibility* Nano#gls("T5") is open-source and available on #link("https://github.com/PiotrNawrot/nanoT5")[Github], promoting transparency and reproducibility in #gls("NLP") research. The framework is designed to be user-friendly, with clear documentation and examples to facilitate adoption by the research community.
The training objective of the framework is the same as the original #gls("T5") model, which is span masking. This means that during training, the model learns to predict masked spans of text within a sequence, allowing it to capture contextual information and relationships between words effectively.
The version of Nano#gls("T5") used in this study is a fork of the original repository by @schneiderkamplab.

== Language Resources From MTD <mtd.sec>
#gls("MTD") Has released a number of resources for the Faroese language. A Github repository @fo_nlp_res contains a collection of datasets and models @mtd_res. \ \ 
A private corpus developed by Uni Johannesen was used as a testset to evaluate the performance of the grammar and spelling models. The corpus contains \~2700 sentences and consists of manually corrected and annotated essays from students. The essays were written in Faroese and contain a variety of errors, including spelling, grammar, and punctuation errors. The testset was originally developed for a thesis to evaluate a #gls("GPT")-4o model on how well it understands faroese grammar and correct errors. Additionally the data used in the study by @Næs on spelling errors in faroese students' essays, was also used in this study. In addition, a private high quality corpus of faroese sentences was provided by #gls("MTD") for training.

== Faroe University Press Papers And Books <fro.sec>
#gls("FRO") has published a number of papers and books in the various fields. The file format of the papers and books is pdf, so some preprocessing is needed to get the text out of them. All publications can be found for free on their #link("https://ojs.setur.fo/index.php/index/index")[website]. The papers are from #gls("FRO"). #gls("FRO") is an annual journal with scientific articles from and about the Faroe Islands and Faroese issues. The journal spans all scientific fields with articles  in Faroese (mostly Humanities and Social Sciences) or English (Natural and Life Sciences). The books are from #gls("FROB"). #gls("FROB")  was established in 2005. It publishes works from all scientific fields in the Faroe Islands. Publications are in Faroese, English and Danish. For this study only the faroese publications have been used.

== Universal Dependencies <ud.sec>
The two Faroese #gls("UD") datasets used in this study are the @oft and @farpahc treebanks. Both datasets consist of manually annotated Faroese sentences. \ \
@oft is based on sentences from the Faroese Wikipedia. The entire Wikipedia was processed using @trond_tool, and sentences containing unknown words were removed. The remaining sentences were manually annotated for Universal Dependencies. Morphological and #gls("POS") tags were converted deterministically using a lookup table, and errors in the original morphology and disambiguation were corrected where found. As is typical for Wikipedia-based corpora, the dataset contains a high proportion of copular constructions and relatively few first- and second-person forms. Additionally, the quality of the original Wikipedia articles may vary, which can impact the consistency and reliability of the syntactic structures found in the data. \ \
@farpahc is a conversion of the @farpahc_og corpus to the Universal Dependencies scheme using @ud_converter. @farpahc_og is a 53,000-word corpus consisting of three texts from the 19th and 20th centuries, originally manually parsed according to the @ppche annotation scheme. Two of these parsed texts were automatically converted to the UD format, resulting in the 40,000-word @farpahc treebank. Due to the historical nature of the source material, the grammar and vocabulary may diverge significantly from contemporary Faroese, which could limit the dataset's applicability to modern language use. \ \
The files with #gls("POS") and #gls("MORPH") labels contain 6,652 Faroese sentences, totaling 9.3 MiB of data. The lemma-labeled files include 1,428 sentences (756 KiB), while the files annotated with dependency parser labels comprise 3,049 sentences, amounting to 2.8 MiB. \ \

== Bendingar.fo <bendingar.sec>
Bendingar.fo is a website, that hosts #gls("BEND") @bend_grunn. #gls("BEND") contains inflections, lemmas and morphological data for faroese words and names. Most words are taken from wordlists that were made for #gls("RÆTT") @rætt, that were taken from a faroese dictionary @fo_ordabók. The names are from the name list from #gls("MAL") @bend_grunn.

== No Language Left Behind <nllb.sec>
The faroese #gls("NLLB") dataset consists of multiple types of data primary #gls("BITEXT"), mined #gls("BITEXT") and monolingual Text. For faroese, the datasets consist of primary @nllb_fo_1 and mined #gls("BITEXT") @nllb_fo_2. The primary #gls("BITEXT") corpora are publicly available parallel corpora from a variety of sources. The mined #gls("BITEXT") corpora are retrieved by large-scale #gls("BITEXT") mining. The mined data includes all the English-centric directions and a subset of non-English-centric directions. Only the faroese data is used in this study. 
