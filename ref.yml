farpahc:
  type: "misc"
  title: "Faroese-FarPaHC Treebank"
  url: 
    value: "https://github.com/UniversalDependencies/UD_Faroese-FarPaHC"
    date: "2024-11-29"

oft:
  type: "misc"
  title: "Faroese-OFT Treebank"
  url: 
   value: "https://github.com/UniversalDependencies/UD_Faroese-OFT"
   date: "2024-11-29"

trond_tool:
  type: "misc"
  title: "Trond Trosterud's tools for Faroese"
  url: 
    value: "https://gtweb.uit.no/cgi-bin/smi/smi.cgi?text=%C3%81+tunguni+eru+sm%C3%A1ar+tenn.&action=analyze&lang=fao&plang=eng"
    date: "2025-02-19"

farpahc_og:
  type: "misc"
  title: "Faroese Parsed Historical Corpus (FarPaHC)"
  url: 
    value: "https://github.com/einarfs/farpahc"
    date: "2025-02-19"

ud_converter:
  type: "misc"
  title: "UDConverter"
  url: 
    value: "https://github.com/thorunna/UDConverter"
    date: "2025-02-19"

ppche:
  type: "misc"
  title: "Penn Parsed Corpora of Historical English"
  url: 
    value: "https://www.ling.upenn.edu/hist-corpora/"
    date: "2025-02-19"

fo_nlp_res:
  type: "misc"
  title: "Faroese Language Resources by FoNLP"
  url: 
    value: "https://github.com/FoNLP/faroese_language_resources"
    date: "2024-12-03"

mtd_res:
  type: "misc"
  title: "Faroese Language Resources by The Centre for Language Technology"
  url: 
    value: "https://mtd.setur.fo/en/tilfeingi/swoof/product_cat-text/"
    date: "2024-12-03"

faroe_uni_press:
  type: "misc"
  title: "Faroe University Press"
  url: 
    value: "https://ojs.setur.fo/"
    date: "2024-12-03"

giellalt:
  type: "misc"
  title: "Giellatekno Language Technology Faroese Resources"
  url: 
    value: "https://github.com/giellalt/corpus-fao/tree/main"
    date: "2024-12-03"

sprotin:
  type: "misc"
  title: "Sprotin"
  url: 
    value: "https://www.sprotin.fo/"
    date: "2024-12-03"

bendingar:
  type: "misc"
  title: "Bendingar"
  url: 
    value: "https://bendingar.fo/"
    date: "2025-02-19"

nllb:
  type: "article"
  title: "No Language Left Behind"
  author:
  - NLLB Team
  - Marta R. Costa-jussà 
  - James Cross
  - Onur Çelebi
  - Maha Elbayad
  - Kenneth Heafield
  - Kevin Heffernan
  - Elahe Kalbassi
  - Janice Lam
  - Daniel Licht
  - Jean Maillard
  - Anna Sun
  - Skyler Wang
  - Guillaume Wenzek
  - Al Youngblood
  - Bapi Akula
  - Loic Barrault
  - Gabriel Mejia Gonzalez
  - Prangthip Hansanti
  - John Hoffman
  - Semarley Jarrett
  - Kaushik Ram Sadagopan
  - Dirk Rowe
  - Shannon Spruit
  - Chau Tran
  - Pierre Andrews
  - Necip Fazil Ayan
  - Shruti Bhosale
  - Sergey Edunov
  - Angela Fan
  - Cynthia Gao
  - Vedanuj Goswami
  - Francisco Guzmán
  - Philipp Koehn
  - Alexandre Mourachko
  - Christophe Ropers
  - Safiyyah Saleem
  - Holger Schwenk
  - Jeff Wang
  url: 
    value: "https://opus.nlpl.eu/NLLB/en&fo/v1/NLLB"
    date: "2025-02-17"
  date: 2022-07-11
  serial-number:
    arxiv: '2207.05267'
    doi: 10.48550/arXiv.2207.04672
  abstract: "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb"
  parent:
    type: periodical
    publisher: arXiv

nllb_fo_1:
  type: "misc"
  title: "NLLB - Faroese - Primary"
  url: 
    value: "https://github.com/facebookresearch/fairseq/blob/nllb/examples/nllb/modeling/scripts/flores200/lang_pairs_primary.txt"
    date: "2025-03-04"

nllb_fo_2:
  type: "misc"
  title: "NLLB - Faroese - Mined"
  url: 
    value: "https://github.com/facebookresearch/fairseq/blob/nllb/examples/nllb/modeling/scripts/flores200/lang_pairs_mine.txt"
    date: "2025-03-04"

Næs:
  type: "article"
  title: "Færøsk retskrivning – resultater fra en undersøgelse af færøske stile"
  author: "Katrin Næs"
  date: "2005"
  url:
    value: "http://ojs.statsbiblioteket.dk/index.php/sin/issue/archive"
    date: "2025-03-10"

spell_errors_src:
  type: "misc"
  title: "Spelling Errors text files"
  url: 
    value: "https://github.com/giellalt/lang-fao/tree/main/tools/spellcheckers"
    date: "2025-03-07"

bend_grunn:
  type: "misc"
  title: "Bendingarunnurin"
  url: 
    value: "https://bendingar.fo/um/"
    date: "2025-02-19"

rætt:
  type: "misc"
  title: "Rættstavarin"
  url: 
    value: "https://divvun.org/"
    date: "2025-02-19"

fo_ordabók:
  type: "book"
  title: "Føroysk Orðabók"
  author: Jóhan Hendrik W. Poulsen
  publisher: Føroya Fróðskaparfelag
  date: "1998"

Vaswani2017:
  type: article
  title: Attention Is All You Need
  author:
  - Vaswani, Ashish
  - Shazeer, Noam
  - Parmar, Niki
  - Uszkoreit, Jakob
  - Jones, Llion
  - Gomez, Aidan N.
  - Kaiser, Lukasz
  - Polosukhin, Illia
  date: 2017-06
  serial-number:
    arxiv: '1706.03762'
    doi: 10.48550/ARXIV.1706.03762
  abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.
  parent:
    type: periodical
    publisher: arXiv



mt5:
  type: article
  title: "mT5: A massively multilingual pre-trained text-to-text transformer"
  author:
  - Linting Xue
  - Noah Constant
  - Adam Roberts
  - Mihir Kale
  - Rami Al-Rfou
  - Aditya Siddhant
  - Aditya Barua
  - Colin Raffel
  date: 2021-05
  serial-number:
    arxiv: '2010.11934v3'
    doi: 10.48550/arXiv.2010.11934
  abstract: The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.
  parent:
    type: periodical
    publisher: arXiv

nawrot:
  type: article
  title: "nanoT5: Fast & Simple Pre-training and Fine-tuning of T5 Models with Limited Resources"
  author: Piotr Nawrot
  date: 2023-12
  url:
    value: "https://aclanthology.org/2023.nlposs-1.11"
    date: 2025-05-12
  serial-number:
    arxiv: '2309.02373'
    doi: 10.48550/arXiv.2309.02373
  publisher: Association for Computational Linguistics
  booktitle: "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)"

